---
title: "Assignment 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Quantitative Methods in Public Policy Assignment 2

Before moving into the body of this assignment, I will install requisite packages and save our data of interest as an object. I also edit a few options to expand the size of resulting figures. Throughout the assignment, I alternative between commentary and relevant code. For convenience the code is compiled at the end of this document as well. 

```{r packages and data}
# configure options for figure aesthetics
knitr::opts_chunk$set(echo = TRUE, fig.width = 12, fig.height = 8)

# install requisite packages
# install.packages(tidyverse)
# install.packages(rvest)
# install.packages(RColorBrewer)
# install.packages(tidytext)
# install.packages(knitr)
# install.packages(downloader)
# install.packages(here)

# save packages to library
library(tidyverse)
library(rvest)
library(RColorBrewer)
library(tidytext)
library(knitr)
library(downloader)
library(here)

# the data analyzed in this assignment concerns the correlation between years of education and earnings of men who were aged between 41 and 50 years. The data can be downloaded via this link. Below is an explainer of the data taken from the read.me.

# "Angrist and Krueger obtained their data from the Integrated Public Use
# Microdata Series from the 1980 U.S. Census.

# The study specifically focused on men born between 1930 and 1939, so they
# were 41-50 years old at the time of data collection.

# Each row corresponds to a large group of men with the same level of
# schooling.

# The variable "schooling" indicates the years of schooling for each group.
# 12 corresponds with high school completion, and 16 corresponds with 
# college completion.

# The variable "earnings" indicates the average annual earnings (in 1980
# U.S. dollars) for the men in each group."

# save the data as an object
schooling_data <- read.csv(here("Files for Data Exercises/SchoolingEarnings.csv"))
  # note here() is taken from the here package. It makes code reproducible across different users and
  # computer systems by creating from the working directory to the file's location. 
  # For instance, running "here("Files for Data Exercises/SchoolingEarnings.csv")" in my console returns
  # "/Users/zanderarnao/Desktop/github projects/Quantitative-Methods/Files for Data
  # Exercises/SchoolingEarnings.csv". 

```

With the data saved and options initialized, we now move into the required questions.

**5.1) Run a regression with earnings as the dependent variable and schooling as the sole independent variable. Interpret the coefficients.** 

Below I run an ordinary least squares regression on the schooling data, resulting in a line of best fit. Here schooling serves as the independent variable and earnings as the dependent. I provide a table with the relevant regression parameters, the regression coefficient (beta) and the intercept (alpha). 

```{r problem 5.1}

# fit a regression line to the schooling data and show a result with regression
regression <- lm(data = schooling_data, earnings ~ schooling)
summary(regression)

```

The regression coefficient and intercept are given in the table above. Beta is 1.16185, which is the slope of the regression line. Its sign is positive, which tells us that there is a positive correlation between years of schooling and earnings, i.e., that, on average, an increase in one variable is associated with an increase in the other.

The specific magnitude of the correlation 1.16185, which means that the middle aged men in our data, each additional year of schooling corresponds, on average, to an increase in income by approximately $1,161.85 (1980 dollars).

Alpha is 8.79853, which tells us that the men in this cohort who received no years of schooling earned, on average, about $8,798.53 in 1980 dollars. 

Now with the specific meaning of the regression parameters for the schooling data, we briefly discuss ... 

**5.2) Suppose you wanted a parsimonious way to predict earnings using only years of schooling. What would you do?**

For a parsimonious way to predict earnings using only years of schooling, I would run an ordinary least squares (OLS) regression of earnings on years of schooling. That is, years of schooling would serve as the explanatory variable and earnings the outcome of interest. Years of schooling would "explain" variation in earnings seen in the data.

Provided that the relationship between the variables is approximately linear, this statistical test would give us a line that, based on years of schooling, summarizes change in earnings. OLS accomplishes this by finding the line which minimizes the total squared error, i.e, the distance between the observed and predicted values of earnings for every year of schooling. 

OLS regression is useful because it is a parsimonious way to communicate the relationship between two or more variables. One could list all the years of schooling and their associated average earnings and draw conclusions about their relationship. However, this method would be time-consuming and heavy with information. 

By contrast, OLS regression allows one to study this relationship without detailed reference to the observed data. OLS regression provides a mathematical model which gives highly useful information, including the direction (positive, negative, or none) of the relationship and a sense of its magnitude. This information can have a number of uses, including for description, prediction, and causal assessment of a number of features within the world. Running an OLS regression between years of schooling and earnings may furnish knowledge about their relationship useful for other contexts (e.g. observations not currently part of our data)

It is also worth briefly comparing OLS regression to higher order regressions. Higher order regressions add additional explanatory variables to the process (e.g. college major as an explainer of income), and while sometimes this can be useful (particularly when the data is the relationship is non-linear), it adds dimensionality and complexity, which can reduce the parsimony of the summary. A regression equation with ten explanatory variables is likely not much more useful than listing or visualizing the data and assessing its 

We can apply it here to find a parsimonious summary of the relationship between year of schooling and earnings. The specific meaning of identified regression coefficient and intercept are elaborated above. We not dig into whether the relationship between years of schooling and earnings is actually linear.  

**5.3) Let’s dig more deeply into whether the relationship between earning and schooling is approximately linear.**

**a) Start by making a scatter plot. Then plot the predicted values from your regression along with the raw data points, as we did in chapter 2. Does the regression line look like it’s fitting the data well?**

First we show a scatter plot of the data. 

```{r scatter plot}

# visualize a scatter plot of the data
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  labs(
    title = "Years of schooling versus earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
```

At first blush, the data appear somewhat linear, so we visualize an OLS regression line on the plot. 

```{r regression line}
# visualize the regression line on a scatter plot
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(color = "purple", method = lm) + 
    labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
```

The OLS regression appears to reasonably fit the data. It is noticeable, however, that over certain intervals the data are uniformly above or below the regression line. This might make us question whether the relationship is linear. 

**b) Now run a fourth-order polynomial regression (i.e., including schooling, schooling squared, schooling to the third, and schooling to the fourth). Do those predictions meaningfully differ from the predictions coming from the linear regression?**

To further elucidate whether the variables are linearly associated, I run a fourth-order polynomial regression on earnings and years of schooling at first, second, third, and fourth powers. I show the regression parameters and a fit a regression line to a scatter plot below. 

```{r fourth order polynomial regression}
# fit a fourth-order regression and show the regression parameters
regression1 <- lm(data = schooling_data, 
  earnings ~ poly(schooling, 4, raw=TRUE))
summary(regression1)

# visualize a fourth-order line of best fit to a scatter plot of the data
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(color = "purple", method = lm, se = TRUE,
              formula = y ~ poly(x, 4, raw=TRUE)) + 
  labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()

````

The predictions meaningfully differ from the linear regression line. For for certain intervals of the data, a linear regression gave predictions that were systematically above or below the observed values. This is not the case for our fourth order polynomial regression. 

In addition, there seems to be less error in the polynomial regression's predictions; the line (and therefore our predicitons) appears uniformly closer to the observed values than the linear regression. 

**c) Now run different regressions for some different ranges of schooling. Do those lines look meaningfully different from the predictions you get from a single regression including all the data?**

We now further test the linearity of the data by regression different ranges of schooling. Comparing the linear regression against the scattered observations, there appear to be three distinct intervals (years 0 to 4, 5 to 15, and 16 to 20). The observations are systematically above, below, and above the predictions made by the line of best fit. 

In addition, the relationship between earnings and schooling  appear to be roughly linear within each of those intervals. Earnings tend to increase very slowly, if at all, from years 0 to 4, then steadily from 5 to 16, and sporadically from 17 to 20. This makes sense because these years correspond to distinct phases of a person's education. Years 0 to 4 are elementary/primary school; 5 to 15 are intermediate, secondary, and college; and 16 to 20 are post-graduate. It is reasonable to think that associations with changes in income will be different across these three groups. 

In light of this, I run a segmented linear regression, that is, I plot a separate line of best fit for each of these intervals. In addition, I show a table with relevant regression parameters and visualize all three lines of best fit on a scatter plot. I start by separating the data into these three groups. 

```{r segement the data}
#separate the data into three groups and append a group id
schooling_g1 <- schooling_data %>%
  filter(schooling < 6) %>% 
  mutate(group = 1)

schooling_g2 <- schooling_data %>%
  filter(5 < schooling & schooling < 16) %>% 
  mutate(group = 2)

schooling_g3 <- schooling_data %>% 
  filter(15 < schooling) %>% 
  mutate(group = 3)

# re-join them for later graphing
schooling_rejoined <- schooling_g1 %>% 
  full_join(schooling_g2) %>% 
  full_join(schooling_g3)

```

Defining three new data sets permits running a linear regression on each. Recombining them facilitates graphing on a common scatter plot. I perform these actions below. 

```{r segmented regression}

# fit a separate linear regression for each group
# for group 1
regression2 <- lm(data = schooling_g1, earnings ~ schooling)
summary(regression2)

# for group 2 
regression3 <- lm(data = schooling_g2, earnings ~ schooling)
summary(regression3)

# for group 3
regression4 <- lm(data = schooling_g3, earnings ~ schooling)
summary(regression4)

# plot segmented linear regression
schooling_rejoined %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(aes(group = group), color = "purple", method = lm) + 
  labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
```
The lines do look meaningfully different from a regression using all of the data. First, the regression line is not continuous, increasing a different rates depending on the year. Second, the predictions appear, in general, to be more accurate. There is no consistent direction of error as occurred in the first linear regression, and the observations appear to be much closer to the lines' predictions, though the fourth order polynomial regression is similarly accurate across the board. It is clear that segmenting the regression improves the accuracy of the predicted outcomes, though at the expense of some parsimony.  

**d) Does all this make you think the simple linear approach was reasonable or unreasonable?**

It makes me think the simple linear approach was unreasonable. The simple linear regression was the most parsimonious since we only have to bear two regression parameters (beta and alpha) in mind. However, this parsimony comes at the expense of accuracy; the predictions made by the linear regression had greater error (and in the same direction over certain intervals) than the fourth order polynomial and segmented regressions. 

It does not appear that years of schooling and earnings are linearly assocaited. 

**5.4) Similar to what we did with age and voter turnout, conduct some out-of-sample tests to evaluate your prediction strategy. Using only data from those with twelve years of schooling or less, see how well your different strategies from question 3 perform when predicting earnings for those with more than twelve years of schooling.**

**5.5). Drop one observation, run a regression to try to predict the outcome for that missing observation, and see how far you were. Repeat this for each observation in the data set (you should be able to do this with a loop) and average your errors. Try different strategies to see which one gives you the best out-of-sample predictions.**


```{r code}

# configure figures for aesthetics
knitr::opts_chunk$set(echo = TRUE, fig.width = 12, fig.height = 8)

# install requisite packages
# install.packages(tidyverse)
# install.packages(rvest)
# install.packages(RColorBrewer)
# install.packages(tidytext)
# install.packages(knitr)
# install.packages(downloader)
# install.packages(here)

# save requisite packages to library
library(tidyverse)
library(rvest)
library(RColorBrewer)
library(tidytext)
library(knitr)
library(downloader)
library(here)

# show explainer of data for future reference (from the associated read_me)
# "Angrist and Krueger obtained their data from the Integrated Public Use
# Microdata Series from the 1980 U.S. Census.

# The study specifically focused on men born between 1930 and 1939, so they
# were 41-50 years old at the time of data collection.

# Each row corresponds to a large group of men with the same level of
# schooling.

# The variable "schooling" indicates the years of schooling for each group.
# 12 corresponds with high school completion, and 16 corresponds with 
# college completion.

# The variable "earnings" indicates the average annual earnings (in 1980
# U.S. dollars) for the men in each group."

# save data to variable
schooling_data <- read.csv(here("Files for Data Exercises/SchoolingEarnings.csv"))
  # note here() removes the need to set a working directory by X

# fit a regression line
regression <- lm(data = schooling_data, earnings ~ schooling)
summary(regression)

# visualize data in a scatter plot with a regression line
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(method = lm)

# fit a fourth-order regression
regression1 <- lm(data = schooling_data, 
  earnings ~ poly(schooling, 4, raw=TRUE))
summary(regression1)

# visualize data in a scatter plot with a fourth-order regression
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(method = lm, se = TRUE,
              formula = y ~ poly(x, 4, raw=TRUE))

#separate data frame into three groups and append group number
schooling_elem <- schooling_data %>%
  filter(schooling < 6) %>% 
  mutate(group = 1)

schooling_college <- schooling_data %>%
  filter(5 < schooling & schooling < 16) %>% 
  mutate(group = 2)

schooling_post <- schooling_data %>% 
  filter(15 < schooling) %>% 
  mutate(group = 3)

schooling_final <- schooling_elem %>% 
  full_join(schooling_college) %>% 
  full_join(schooling_post)

# fit a segmented regression
# for group 1
regression2 <- lm(data = schooling_elem, earnings ~ schooling)
summary(regression2)

# for group 2 
regression3 <- lm(data = schooling_college, earnings ~ schooling)
summary(regression3)

# for group 3
regression4 <- lm(data = schooling_post, earnings ~ schooling)
summary(regression4)

# plot segmented linear regression
schooling_final %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(aes(group = group), method = lm)

# create in and out of sample data
in_sample <- schooling_data %>% 
  filter(schooling < 13)

out_of_sample <- schooling_data %>% 
  filter(12 < schooling)

# fit linear regression to in_sample data 
regression5 <- lm(data = in_sample, earnings ~ schooling)
summary(regression5)

# plot linear regression from in sample data showing out of sample data
ggplot() + 
  geom_point(data = in_sample, 
    aes(x = schooling, y = earnings), color = "green") + 
  geom_point(data = out_of_sample, 
    aes(x = schooling, y = earnings), color = "red") + 
  geom_function(fun = function(x) 0.70967 * x + 11.07127)
  

# fit fourth order regression to in_sample data
regression6 <- lm(data = in_sample, 
  earnings ~ poly(schooling, 4, raw=TRUE))
summary(regression6)

# plot fourth-order regression from in sample data showing out of sample data
ggplot() + 
  geom_point(data = in_sample, 
    aes(x = schooling, y = earnings), color = "green") + 
  geom_point(data = out_of_sample, 
    aes(x = schooling, y = earnings), color = "red") + 
  geom_function(fun = function(x) -0.5514305 * x + 
                  0.2408554 * x^2 + 
                  -0.0185683 * x^3 + 
                  0.0005741 * x^4 +
                  12.6042321)

#separate in_sample data into two groups and append group number
in_sample_g1 <- in_sample %>%
  filter(schooling < 4) %>% 
  mutate(group = 1)

in_sample_g2 <- in_sample %>%
  filter(3 < schooling) %>% 
  mutate(group = 2)

in_sample_segmented <- in_sample_g1 %>% 
  full_join(in_sample_g2) 

# fit a segmented regression to in_sample data
# for group 1
regression7 <- lm(data = in_sample_g1, earnings ~ schooling)
summary(regression7)

# for group 2 
regression8 <- lm(data = in_sample_g2, earnings ~ schooling)
summary(regression8)

# plot segmented linear regression showing out of sample data
ggplot() + 
  geom_point(data = in_sample, 
    aes(x = schooling, y = earnings), color = "green") + 
  geom_point(data = out_of_sample, 
    aes(x = schooling, y = earnings), color = "red") + 
  geom_segment(aes(x = 0, y = 12.41659, xend = 3, yend = 12.45874)) + 
  geom_segment

# calculate average error using a linear and fourth-order regression

# Where to store the square errors for model 1?
errors1 <- c()

# Where to store the square errors for model 2?
errors2 <- c()

# for loop
for (i in c(1:21)){
  
  ### Lets split our data into training and testing
  training <- schooling_data[-i, ]
  testing <- schooling_data[i, ]
  
  ### Model 1
  linear = lm(earnings ~ schooling, data = training)
  temp <- (testing$earnings - predict(linear, testing))^2
  errors1 <- c(errors1, temp)
  
  ### Model 2
  # now re-estimate model 2 with the new training data
  fourth_order = lm(earnings ~ poly(schooling, 4, raw=TRUE), data = training)
  temp <- (testing$earnings - predict(fourth_order, testing))^2
  errors2 <- c(errors2, temp)
  
}

# Now we can see which model has smaller mean squared errors on average
mean(errors1)
mean(errors2)

```
