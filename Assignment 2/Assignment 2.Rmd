---
title: "Quantitative Methods in Public Policy Assignment 2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Below I install requisite packages and save our data. Note: I chose not to compile all my code at the end so as to not make the document super long. 

```{r packages and data, message = FALSE}

# install requisite packages
# install.packages(tidyverse)
# install.packages(rvest)
# install.packages(RColorBrewer)
# install.packages(knitr)
# install.packages(here)

# save packages to library
library(tidyverse)
library(rvest)
library(RColorBrewer)
library(knitr)
library(here)

# the data analyzed in this assignment concerns the correlation
# between years of education and earnings of men who were 
# aged between 41 and 50 years in 1980. 
# The data can be downloaded via this link: 
# https://press.princeton.edu/student-resources/thinking-clearly-with-data.


# save the data as an object
schooling_data <- read.csv(here("Files for Data Exercises/SchoolingEarnings.csv"))
  # note here() is taken from the here package. 
  # It makes code reproducible across different users and
  # computer systems by creating a path from the working directory to the file's location. 
  # For instance, running 
  # "here("Files for Data Exercises/SchoolingEarnings.csv")" 
  # in my console returns
  # "/Users/zanderarnao/Desktop/github projects/Quantitative-Methods/Files for Data
  # Exercises/SchoolingEarnings.csv". 

```

**5.1) Run a regression with earnings as the dependent variable and schooling as the sole independent variable. Interpret the coefficients.** 

Below I run an ordinary least squares (OlS) regression on the schooling data, resulting in the line of best fit visualized in the scatter plot. Here schooling serves as the independent variable and earnings as the dependent. I provide a table with the relevant regression parameters: the regression coefficient (beta) and the intercept (alpha). 

```{r problem 5.1}

# fit a regression line to the schooling data
regression <- lm(data = schooling_data, earnings ~ schooling)
summary(regression)

```

The regression coefficient and intercept are given in the table above. Beta is 1.16185, which is the slope of the regression line. Its sign is positive, which tells us that there is a positive correlation between years of schooling and earnings, i.e., that, on average, an increase in one variable is associated with an increase in the other.

The specific magnitude of the correlation 1.16185, which means that for the middle aged men in our data, each additional year of schooling corresponds, on average, to an increase in income by approximately $1,161.85 (in 1980 dollars).

Alpha is 8.79853, which tells us that the men in this cohort who received no years of schooling earned, on average, about $8,798.53 (in 1980 dollars). 

Now with the specific meaning of the regression parameters for the schooling data, we briefly discuss why we would run an OLS regression. 

**5.2) Suppose you wanted a parsimonious way to predict earnings using only years of schooling. What would you do?**

For a parsimonious way to predict earnings using only years of schooling, I would run an ordinary least squares (OLS) regression of earnings on years of schooling.Provided that the relationship between the variables is approximately linear, this statistical test would give us a line that, based on years of schooling, summarizes change in earnings on average. OLS accomplishes this by finding the line which minimizes the total squared errors, i.e, the squared sum of the all distances between the observed and predicted values of earnings for every year of schooling. 

OLS regression is useful because it is a parsimonious way to communicate the relationship between two or more variables. Alternatively, one could list all the years of schooling and their associated average earnings and draw conclusions about their relationship, but this method would be time-consuming and heavy with tedious information. 

By contrast, OLS regression allows one to study this relationship without detailed reference to the observed data. OLS regression provides a mathematical model which gives highly useful information, including the direction (positive, negative, or none) of the relationship and a sense of its magnitude. 

It is also worth briefly comparing OLS regression to higher order regressions. Higher order regressions add additional explanatory variables to the process (e.g., college major as an additional explainer of income), and while this can be useful (particularly when the relationship relationship is non-linear), it adds dimensionality and complexity that reduces the parsimony of the summary.

We can apply it here to find a parsimonious summary of the relationship between year of schooling and earnings. I do that below.

**5.3) Let’s dig more deeply into whether the relationship between earning and schooling is approximately linear.**

**a) Start by making a scatter plot. Then plot the predicted values from your regression along with the raw data points, as we did in chapter 2. Does the regression line look like it’s fitting the data well?**

First we show a scatter plot of the data. 

```{r scatter plot}

# visualize a scatter plot of the data
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  labs(
    title = "Years of schooling versus earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
```

At first blush, the data appear somewhat linear, so we visualize an OLS regression line on the plot. 

```{r regression line}
# visualize the regression line on a scatter plot
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(color = "purple", method = lm) + 
    labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()

# note: I do not add legends in this assignment. 
# The graphics are fairly simple, which makes the time it would take to code
# a legend for each unnecessary. 
```

The OLS regression appears to reasonably fit the data. It is noticeable, however, that over certain intervals the data are uniformly above or below the regression line. That means the line is systematically over- or underestimating the actual outcomes for stretches of the data. This make me question whether the relationship is linear. 

**b) Now run a fourth-order polynomial regression (i.e., including schooling, schooling squared, schooling to the third, and schooling to the fourth). Do those predictions meaningfully differ from the predictions coming from the linear regression?**

To further elucidate whether the variables are linearly associated, I run a fourth-order polynomial regression on earnings and years of schooling at first, second, third, and fourth powers. I show the regression parameters and a fit a regression line to a scatter plot below. 

```{r fourth order polynomial regression}
# fit a fourth-order regression and summarize the regression parameters
regression1 <- lm(data = schooling_data, 
  earnings ~ poly(schooling, 4, raw=TRUE))
summary(regression1)

# visualize a fourth-order line of best fit to a scatter plot of the data
schooling_data %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(color = "purple", method = lm,
              formula = y ~ poly(x, 4, raw=TRUE)) + 
  labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()

````

The predictions meaningfully differ from the linear regression. For for certain intervals of the data, a linear regression gave predictions that were systematically above or below the observed values. This is not the case for our fourth order polynomial regression. 

In addition, there seems to be less error in the polynomial regression's predictions; the line (and therefore its predictions) appears uniformly closer to the observed values than the linear regression. 

**c) Now run different regressions for some different ranges of schooling. Do those lines look meaningfully different from the predictions you get from a single regression including all the data?**

Comparing the linear regression against the scattered observations, there appear to be three distinct intervals (years 0 to 5, 6 to 15, and 16 to 20). The observations are systematically above, below, and above the predictions made by the line of best fit in each of these intervals. In addition, the relationship between earnings and schooling  appear to be roughly linear within each of those intervals.

In light of this, I run a segmented linear regression on each of these intervals. I start by separating the data into these three groups. 

```{r segement the data}
#separate the data into three groups and append a group id
schooling_g1 <- schooling_data %>%
  filter(schooling < 6) %>% 
  mutate(group = 1)

schooling_g2 <- schooling_data %>%
  filter(5 < schooling & schooling < 16) %>% 
  mutate(group = 2)

schooling_g3 <- schooling_data %>% 
  filter(15 < schooling) %>% 
  mutate(group = 3)

# re-join them for later graphing
schooling_rejoined <- schooling_g1 %>% 
  full_join(schooling_g2) %>% 
  full_join(schooling_g3)

```

```{r segmented regression}

# fit a separate linear regression for each group
# for group 1
regression2 <- lm(data = schooling_g1, earnings ~ schooling)
summary(regression2)

# for group 2 
regression3 <- lm(data = schooling_g2, earnings ~ schooling)
summary(regression3)

# for group 3
regression4 <- lm(data = schooling_g3, earnings ~ schooling)
summary(regression4)

# plot segmented linear regression
schooling_rejoined %>% 
  ggplot(aes(x = schooling, y = earnings)) + 
  geom_point() + 
  geom_smooth(aes(group = group), color = "purple", method = lm) + 
  labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
```


**d) Does all this make you think the simple linear approach was reasonable or unreasonable?**

It makes me think the simple linear approach was unreasonable. The simple linear regression was the most parsimonious since we only have to bear two regression parameters (beta and alpha) in mind. However, this parsimony comes at the expense of accuracy; the predictions made by the linear regression had greater error (which was the same direction over certain intervals) than the fourth order polynomial and segmented regressions. 

It does not appear, therefore, that the relationship between years of schooling and earnings is approximately linear. While there is certainly a positive correlation, i.e. that earnings tends to increase with years of schooling on average, the magnitude of that change differs over time. Linearity implies an approximately constant rate of change. 

**5.4) Similar to what we did with age and voter turnout, conduct some out-of-sample tests to evaluate your prediction strategy. Using only data from those with twelve years of schooling or less, see how well your different strategies from question 3 perform when predicting earnings for those with more than twelve years of schooling.**

I now compare the efficacy of alternative strategies to the simple linear approach. I will fit one simple linear, one fourth order polynomial, and one segmented linear regression to only the observations with 12 or less years of schooling. I then test their predictions for observations with greater than 12 years of age. 

I start by splitting the data into in sample and out of sample groups. 
```{r split data}
# create in and out of sample data
in_sample <- schooling_data %>% 
  filter(schooling < 13)

out_of_sample <- schooling_data %>% 
  filter(12 < schooling)

```

Now, I fit a simple linear model to the in sample data and test its predictions against the out of sample.

```{r simple linear out of sample test}
# fit a simple linear regression to in sample data 
regression5 <- lm(data = in_sample, earnings ~ schooling)
summary(regression5)

# plot linear regression from in sample data showing out of sample data
ggplot() + 
  geom_point(data = in_sample, 
    aes(x = schooling, y = earnings), color = "blue") + 
  geom_point(data = out_of_sample, 
    aes(x = schooling, y = earnings), color = "red") + 
  geom_function(fun = function(x) 0.70967 * x + 11.07127, color = "purple") + 
   labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
  
```
It appears that a simple linear model fitted to the first twelve years of schooling is horribly predictive of out of sample data. While there is less error from years zero to 12 than there was along the simple linear regression of the entire data set, the observations are much higher than predicted when assuming linearity. This is further confirmation of my conclusion.   

Below I fit a fourth order regression to the in sample data and test its predictions against the out of sample. In the visualizations below, in sample observations are plotted in blue and out of sample observations are plotted in red. 

```{r fourth order out of sample test}
# fit fourth order regression to in sample data
regression6 <- lm(data = in_sample, 
  earnings ~ poly(schooling, 4, raw=TRUE))
summary(regression6)

# plot fourth-order regression from in sample data showing out of sample data
ggplot() + 
  geom_point(data = in_sample, 
    aes(x = schooling, y = earnings), color = "blue") + 
  geom_point(data = out_of_sample, 
    aes(x = schooling, y = earnings), color = "red") + 
  geom_function(fun = function(x) -0.5514305 * x + 
                  0.2408554 * x^2 + 
                  -0.0185683 * x^3 + 
                  0.0005741 * x^4 +
                  12.6042321, 
                color = "purple") + 
  labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()
```
The fourth order polynomial regression has much less error than the simple linear model. While the out of sample predictions are not amazingly accurate, they are much closer to the observed outcomes than the simple linear model. In addition, they are not systematically above or below observed outcomes as they were with the simple linear model. This suggests a polynomial regression might be an effective strategy.   

Finally, I test a segmented linear regressions over two intervals (from years 0 to 3 and group 2 from years 4 to 12). I pick these intervals because the rate of change appears to differ between years 3 and 4, though perhaps 4 and 5 would also serve as credible cut offs.

```{r segmented out of sample test}
#separate in_sample data into two groups and append group number
in_sample_g1 <- in_sample %>%
  filter(schooling < 4) %>% 
  mutate(group = 1)

in_sample_g2 <- in_sample %>%
  filter(3 < schooling) %>% 
  mutate(group = 2)

# recombine them for graphing
in_sample_segmented <- in_sample_g1 %>% 
  full_join(in_sample_g2) 

# fit a segmented regression to in_sample data
# for group 1
regression7 <- lm(data = in_sample_g1, earnings ~ schooling)
summary(regression7)

# for group 2 
regression8 <- lm(data = in_sample_g2, earnings ~ schooling)
summary(regression8)

# plot segmented linear regression showing out of sample data
ggplot() + 
  geom_point(data = in_sample, 
    aes(x = schooling, y = earnings), color = "blue") + 
  geom_point(data = out_of_sample, 
    aes(x = schooling, y = earnings), color = "red") + 
  geom_segment(aes(x = 0, y = 12.41659, xend = 3, yend = 12.45874), 
               color = "orange") + 
  geom_segment(aes(x = 4, y = 13.01128, xend = 20, yend = 27.42408), 
               color = "purple") + 
   labs(
    title = "Association between years of schooling and earned income",
    x = "Years of Schooling",
    y = "Earnings") + 
  scale_fill_brewer(palette = "Set3") +
  theme_bw()

# note: the group 1 regression line is shown in orange; group 2's is purple. 

```


**5.5). Drop one observation, run a regression to try to predict the outcome for that missing observation, and see how far you were. Repeat this for each observation in the data set (you should be able to do this with a loop) and average your errors. Try different strategies to see which one gives you the best out-of-sample predictions.**



```{r}
# calculate average error using a linear and first through fifth order regressions

# initialize an empty character vector
errors1 <- c()

# initialize an empty character vector
errors2 <- c()

# initialize an empty character vector
errors3 <- c()

# initialize an empty character vector
errors4 <- c()

# initialize an empty character vector
errors5 <- c()

# for loop
for (i in c(1:21)){
  
  # split the data into training and testing
  training <- schooling_data[-i, ]
  testing <- schooling_data[i, ]
  
  # simple linear model
  linear = lm(earnings ~ schooling, data = training)
  temp <- (testing$earnings - predict(linear, testing))^2
  errors1 <- c(errors1, temp)
  
  # second order model
  second_order = lm(earnings ~ poly(schooling, 2, raw=TRUE), data = training)
  temp <- (testing$earnings - predict(second_order, testing))^2
  errors2 <- c(errors2, temp)
  
  # third order model 
  third_order = lm(earnings ~ poly(schooling, 3, raw=TRUE), data = training)
  temp <- (testing$earnings - predict(third_order, testing))^2
  errors3 <- c(errors3, temp)
  
  # fourth order model 
  fourth_order = lm(earnings ~ poly(schooling, 4, raw=TRUE), data = training)
  temp <- (testing$earnings - predict(fourth_order, testing))^2
  errors4 <- c(errors4, temp)
  
  # sith order model (did you catch that one?) 
  fifth_order = lm(earnings ~ poly(schooling, 5, raw=TRUE), data = training)
  temp <- (testing$earnings - predict(fifth_order, testing))^2
  errors5 <- c(errors5, temp)
  
}

# She the mean squared error for each model.
mean(errors1)
mean(errors2)
mean(errors3)
mean(errors4)
mean(errors5)

```


